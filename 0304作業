Cross-Entropy(交叉熵)
定義：H(P,Q) = −∑P(x)logQ(x) ； P是真實標籤，Q是預測機率。
交叉熵要用在分類問題中，幫助我們判斷模型的預測準確度。
如果模型的預測結果和實際差距越大，交叉熵的值就越大，表示模型的錯誤越大；反之，如果預測很準確，交叉熵就會越小。

KL Divergence(相對熵)
定義：KL(P∣∣Q) = H(P,Q)−H(P) ； H(P,Q)是交叉熵，H(P)是真實分佈的熵。
相對熵用來衡量模型預測(Q)真實分佈(P)之間的差距，當Q越接近P時，相對熵越小，而交叉熵就越接近真實分佈的熵。

兩者差異(GPT輔助總結)
交叉熵是直接評估分類模型預測的準確度，主要用於監督學習中的分類問題，直接計算模型預測與標籤之間的誤差，幫助模型調整參數。
相對熵是衡量兩個機率分佈的相似程度，適用於機率模型學習與機率分佈的優化。
