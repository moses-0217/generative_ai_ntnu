Cross-Entropy(交叉熵)
定義：H(P,Q) = −∑P(x)logQ(x)
交叉熵要用在分類問題中，幫助我們判斷模型的預測準確度。
如果模型的預測結果和實際差距越大，交叉熵的值就越大，表示模型的錯誤越大；反之，如果預測很準確，交叉熵就會越小。

KL Divergence(相對熵)
定義：\[
D_{KL}(P || Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}
\]
